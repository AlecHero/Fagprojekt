{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from New_grass import weighted_grassmannian_clustering\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import make_interp_spline\n",
    "import pickle\n",
    "import scipy "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:24px;\">Loading the eigenvects and clustering data using grassmann clustering</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "# Only dealing with data from sleep_run's for each subject\n",
    "# All data has sample size 429\n",
    "cluster_numbers = [2, 3, 4, 5, 6, 8, 10, 12, 15]\n",
    "number_of_subjects = 33 \n",
    "run_number = [2, 3, 4, 5] # sleep-run-numbers\n",
    "all_data_matrices = {} # lib to hold all the sleep_stage data for each subject\n",
    "valid_data_lib = {} # holds information about each subject and for what sleep stage runs we have data // valid_data_lib['01] - subject 1\n",
    "\n",
    "for subject in range(1, number_of_subjects + 1):\n",
    "    subject = str(subject).zfill(2) # fill 01, 02 and so on\n",
    "\n",
    "    all_data_matrices[subject] = {}\n",
    "    valid_data_lib[subject] = []\n",
    "\n",
    "    for number in run_number:\n",
    "        data_matrix = []\n",
    "        with open(f\"sleep_data/sleep_scores/sub-{subject}-sleep-stage.tsv\", \"r\", newline=\"\", encoding=\"utf-8\") as tsv_file: # needs to do this for every number othervise it does not work\n",
    "            tsv_reader = csv.reader(tsv_file, delimiter=\"\\t\")\n",
    "            next(tsv_reader) # skip first row to remove column names\n",
    "            for row in tsv_reader:\n",
    "                if subject != \"01\":\n",
    "                    if row[0] == f'task-sleep_run-{number}': \n",
    "                        data_matrix.append(row)\n",
    "                else:\n",
    "                    if row[1] == f'task-sleep_run-{number}': # We do this to handle the case where the first collumn is not 'task-sleep_run-x' as is the case for subject 1\n",
    "                        data_matrix.append(row)   \n",
    "            if len(data_matrix)>0:        \n",
    "                all_data_matrices[subject][number] = data_matrix\n",
    "                valid_data_lib[subject].append(number)\n",
    "\n",
    "for number in run_number: # removing first collumn in data for subject \"01\" as it contains an extra collumn compared to the rest of the data\n",
    "    try: \n",
    "        all_data_matrices[\"01\"][number] = [row[1:] for row in all_data_matrices[\"01\"][number]]\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "for subject in all_data_matrices: # removing subjects with no data for sleep stages at all\n",
    "    numbers_to_remove = []\n",
    "    for number in valid_data_lib[subject]: # removing sleep stages that contain a constant state \n",
    "        if len(np.unique([s[0] for s in np.array(all_data_matrices[subject][number])[:, 2]])) == 1:\n",
    "            numbers_to_remove.append(number)\n",
    "    for number in numbers_to_remove:\n",
    "        valid_data_lib[subject].remove(number)\n",
    "    if valid_data_lib[subject] == []:\n",
    "        valid_data_lib.pop(subject)\n",
    "\n",
    "# removing some subjects manually if they seem to contain to much noise or are awake almost all the time - this will not be caught above\n",
    "valid_data_lib.pop('11')\n",
    "valid_data_lib.pop('23')\n",
    "valid_data_lib.pop('27')\n",
    "valid_data_lib.pop('20')\n",
    "valid_data_lib.pop('03')\n",
    "valid_data_lib.pop('07')\n",
    "\n",
    "valid_data_lib = {key: value for key, value in valid_data_lib.items() if len(value) >= len(run_number)} # finally we remove all subjects where we dont have valid data for all (4) runs\n",
    "\n",
    "print(len(valid_data_lib)) # number of subjects left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22308, 116, 2) 22308\n"
     ]
    }
   ],
   "source": [
    "# colleting all data an using vstack to get correct shape\n",
    "all_data = []\n",
    "\n",
    "for subject in valid_data_lib:\n",
    "    for number in valid_data_lib[subject]:\n",
    "        with h5py.File(f\"sleep_data/eigvecs/sub-{subject}_session-task-sleep_run-{number}_eigvecs.h5\", \"r\") as file:\n",
    "            data = file['eigvecs'][:]\n",
    "            all_data.append(data)\n",
    "\n",
    "# Stack all the data vertically\n",
    "all_data = np.vstack(all_data)\n",
    "\n",
    "print(all_data.shape, 13*429*4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22308, 2) 22308\n"
     ]
    }
   ],
   "source": [
    "#eigenvals\n",
    "eigenvals = []\n",
    "for subject in valid_data_lib:\n",
    "    for number in valid_data_lib[subject]:\n",
    "        with h5py.File(f\"sleep_data/eigvals/sub-{subject}_session-task-sleep_run-{number}_eigvals.h5\", \"r\") as file:\n",
    "            data = file['eigvals'][:]\n",
    "            eigenvals.append(data)\n",
    "eigenvals = np.vstack(eigenvals)\n",
    "\n",
    "print(eigenvals.shape, 13*429*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diametrical_clustering_plusplus(X,K):\n",
    "    \"\"\"\n",
    "    Diametrical clustering plusplus - initialization strategy for diametrical clustering.\n",
    "    Input:\n",
    "        X: data matrix (n,p)\n",
    "        K: number of clusters\n",
    "    Output:\n",
    "        C: cluster centers\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    n,p,q = X.shape\n",
    "\n",
    "    # choose first centroid at random from X\n",
    "    idx = np.random.choice(n,p,q=None)\n",
    "    C = X[idx][:,np.newaxis]\n",
    "    X = np.delete(X,idx,axis=0)\n",
    "\n",
    "    # for all other centroids, compute the distance from all X to the current set of centroids. \n",
    "    # Construct a weighted probability distribution and sample using this. \n",
    "\n",
    "    for k in range(K-1):\n",
    "        dist = 1-(X@C)**2 #large means far away\n",
    "        min_dist = np.min(dist,axis=1) #choose the distance to the closest centroid for each point\n",
    "        prob_dist = min_dist/np.sum(min_dist) # construct the prob. distribution\n",
    "        idx = np.random.choice(n-k-1,p=prob_dist)\n",
    "        C = np.hstack((C,X[idx][:,np.newaxis]))\n",
    "        X = np.delete(X,idx,axis=0)\n",
    "    \n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_grassmannian_clustering(X,X_weights,K,number_of_subjects,max_iter=10000,tol=1e-16):\n",
    "    \"\"\"\"\n",
    "    Weighted grassmannian clustering using the chordal distance function and a SVD-based update rule\n",
    "    \n",
    "    X: size (nxpxq), where n is the number of observations, p is the number of features and q is the subspace dimensionality\n",
    "    X_weights: size (n,q), where n is the number of observations and q is the subspace dimensionality (corresponds to eigenvalues)\n",
    "    K: number of clusters\n",
    "    max_iter: maximum number of iterations\n",
    "    tol: tolerance for convergence\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    n,p,q = X.shape\n",
    "\n",
    "    # initialize cluster centers using a normal distribution projected to the Grassmannian\n",
    "    C = np.random.randn(K,p,q)\n",
    "    # C = diametrical_clustering_plusplus(X,K)\n",
    "    C_weights = np.ones((K,q))\n",
    "    for k in range(K):\n",
    "        C[k] = C[k]@scipy.linalg.sqrtm(np.linalg.inv(C[k].T@C[k])) # project onto the Grassmannian\n",
    "\n",
    "    # initialize counters\n",
    "    data_number = 429\n",
    "    sub_obj = []\n",
    "    iter = 0\n",
    "    obj = []\n",
    "    partsum = np.zeros((max_iter,K))\n",
    "    while True:\n",
    "        # \"E-step\" - compute the similarity between each matrix and each cluster center\n",
    "        dis = 1/np.sqrt(2)*(np.sum(X_weights**4)+np.sum(C_weights**4)-2*np.linalg.norm(np.swapaxes((X*X_weights[:,None,:])[:,None],-2,-1)@(C*C_weights[:,None,:])[None],axis=(-2,-1)))\n",
    "        sim = -dis\n",
    "        maxsim = np.max(sim,axis=1) # find the maximum similarity - the sum of this value is the objective function\n",
    "        X_part = np.argmax(sim,axis=1) # assign each point to the cluster with the highest similarity\n",
    "        obj.append(np.sum(maxsim))\n",
    "        part_list = []\n",
    "        for idx in range(number_of_subjects):\n",
    "            part_list.append(np.sum(maxsim[idx*data_number : (idx+1)*data_number]))\n",
    "        sub_obj.append(part_list)\n",
    "\n",
    "        # check for convergence\n",
    "        for k in range(K):\n",
    "            partsum[iter,k] = np.sum(X_part==k)\n",
    "        if iter>0:\n",
    "            if all((partsum[iter-1]-partsum[iter])==0) or iter==max_iter or abs(obj[-1]-obj[-2])<tol:\n",
    "                break\n",
    "        \n",
    "        # \"M-step\" - update the cluster centers\n",
    "        for k in range(K):\n",
    "            idx_k = X_part==k\n",
    "            # if np.sum(idx_k) == 0:\n",
    "            #     continue # this works, but i suspect we lose a lot of information\n",
    "            V = np.reshape(np.swapaxes(X[idx_k]*X_weights[idx_k,None,:],0,1),(p,np.sum(idx_k)*q))\n",
    "            U,S,_ = scipy.sparse.linalg.svds(V,q)\n",
    "            C[k] = U[:,:q]\n",
    "            C_weights[k] = S**2\n",
    "\n",
    "        iter += 1\n",
    "    \n",
    "    return C,obj,X_part,sub_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m cluster \u001b[39min\u001b[39;00m tqdm(cluster_numbers):\n\u001b[1;32m      4\u001b[0m     cluster_assignments_objfunc_cent[cluster] \u001b[39m=\u001b[39m {}\n\u001b[0;32m----> 5\u001b[0m     C, obj, part, sub_obj \u001b[39m=\u001b[39m weighted_grassmannian_clustering(all_data, eigenvals, cluster, \u001b[39mlen\u001b[39;49m(valid_data_lib), max_iter\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m     cluster_assignments_objfunc_cent[cluster][\u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m C\n\u001b[1;32m      7\u001b[0m     cluster_assignments_objfunc_cent[cluster][\u001b[39m'\u001b[39m\u001b[39mobj\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m obj[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m# tager kun den bedste obj value efter grassmann terminater\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[25], line 17\u001b[0m, in \u001b[0;36mweighted_grassmannian_clustering\u001b[0;34m(X, X_weights, K, number_of_subjects, max_iter, tol)\u001b[0m\n\u001b[1;32m     13\u001b[0m n,p,q \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape\n\u001b[1;32m     15\u001b[0m \u001b[39m# initialize cluster centers using a normal distribution projected to the Grassmannian\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m# C = np.random.randn(K,p,q)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m C \u001b[39m=\u001b[39m diametrical_clustering_plusplus(X,K)\n\u001b[1;32m     18\u001b[0m C_weights \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones((K,q))\n\u001b[1;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(K):\n",
      "Cell \u001b[0;32mIn[24], line 12\u001b[0m, in \u001b[0;36mdiametrical_clustering_plusplus\u001b[0;34m(X, K)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdiametrical_clustering_plusplus\u001b[39m(X,K):\n\u001b[1;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m    Diametrical clustering plusplus - initialization strategy for diametrical clustering.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m    Input:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     n,_ \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape\n\u001b[1;32m     14\u001b[0m     \u001b[39m# choose first centroid at random from X\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(n,p\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "cluster_assignments_objfunc_cent = {}\n",
    "\n",
    "for cluster in tqdm(cluster_numbers):\n",
    "    cluster_assignments_objfunc_cent[cluster] = {}\n",
    "    C, obj, part, sub_obj = weighted_grassmannian_clustering(all_data, eigenvals, cluster, len(valid_data_lib), max_iter=500)\n",
    "    cluster_assignments_objfunc_cent[cluster]['C'] = C\n",
    "    cluster_assignments_objfunc_cent[cluster]['obj'] = obj[-1] # tager kun den bedste obj value efter grassmann terminater\n",
    "    cluster_assignments_objfunc_cent[cluster]['part'] = part\n",
    "    cluster_assignments_objfunc_cent[cluster]['obj_sub'] = sub_obj[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary to a file\n",
    "# with open('cluster_assignments_weighted_objfunc.pkl', 'wb') as f:\n",
    "#     pickle.dump(cluster_assignments_objfunc_cent, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:24px;\">Using Objective Function Value as meassure for preformance</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:24px;\">Using NMI as meassure for preformance</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:24px;\">1 out of K encoding of the true labels</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:24px;\">1 out of K encoding of predicted labels using grassmann clustering</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:24px;\">Calculating NMI</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 08:08:27) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3dc8e3e4ff9c8fdbed109d8133de8d8f695e288fe9361851b36cbf17bb1975b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
